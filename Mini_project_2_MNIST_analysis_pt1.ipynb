{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project #2: MNIST Analysis\n",
    "\n",
    "An easy-to-follow scikit-learn tutorial that will help you to get started with the Python machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Python\n",
    "\n",
    "Machine learning is a branch in computer science that studies the design of algorithms that can learn.\n",
    "\n",
    "Typical tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. These tasks are learned through available data that were observed through experiences or instructions, for example.\n",
    "\n",
    "The hope that comes with this discipline is that including the experience into its tasks will eventually improve the learning. But this improvement needs to happen in such a way that the learning itself becomes automatic so that humans like ourselves don’t need to interfere anymore is the ultimate goal.\n",
    "\n",
    "Today’s scikit-learn tutorial will introduce you to the basics of Python machine learning:\n",
    "\n",
    "- Part 1: You'll learn how to use Python and its libraries to explore your data with the help of matplotlib and Principal Component Analysis (PCA),\n",
    "- Part 2a: And you'll preprocess your data with normalization and you'll split your data into training and test sets.\n",
    "- Part 2b: Next, you'll work with the well-known KMeans algorithm to construct an unsupervised model, fit this model to your data, predict values, and validate the model that you have built.\n",
    "- Part 3: As an extra, you'll also see how you can also use Support Vector Machines (SVM) to construct another model to classify your data.\n",
    "\n",
    "Let's start with Part 1 now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your Data Set\n",
    "The first step to about anything in data science is loading in your data. This is also the starting point of this scikit-learn tutorial.\n",
    "\n",
    "This discipline typically works with observed data. This data might be collected by yourself or you can browse through other sources to find data sets. But if you’re not a researcher or otherwise involved in experiments, you’ll probably do the latter.\n",
    "\n",
    "If you’re new to this and you want to start problems on your own, finding these data sets might prove to be a challenge. However, you can typically find good data sets at the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets) or on the [Kaggle](https://www.datacamp.com/community/tutorials/www.kaggle.com) website. Also, check out this [KD Nuggets list with resources](http://www.kdnuggets.com/datasets/index.html).\n",
    "\n",
    "__NOTE__: the MNIST(DIGIT) dataset is one of the most famous data sets in the machine learning context - if you are going to take BA 550, you will see this dataset in deep learning again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the datasets module contains other methods to load and fetch popular reference datasets, and you can also count on this module in case you need artificial data generators. In addition, this data set is also available through the UCI Repository that was mentioned above: you can find the data [here](http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/).\n",
    "\n",
    "If you would have decided to pull the data from the latter page, your data import would’ve looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the `pandas` library as `pd`\n",
    "\n",
    "\n",
    "# Load in the data with `read_csv()`\n",
    "digits = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\", header=None)\n",
    "\n",
    "# since `digits` is now a dataframe (without the meta data), we can just look at the first five rows using .head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you download the data like this, the data is already split up in a __training__ and a __test set__, indicated by the extensions __.tra__ and __.tes__. You’ll need to load in both files to elaborate your project. With the command above, you only load in the __training set__.\n",
    "\n",
    "Please write your own code below to download the __test set__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test set URL: http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes\n",
    "# use `read_csv()` again, and name the dataframe as `digits_test`\n",
    "\n",
    "\n",
    "# you can check the first 5 rows of `digit_test` to make sure you have downloaded in correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of `scikit-learn`, you do not have to download any data since the DIGITS dataset is pre-loaded in it. You can just load it from `scikit-learn`.\n",
    "\n",
    "Fun fact: did you know the name originates from the fact that this library is a scientific toolbox built around SciPy? By the way, there is [more than just one scikit](https://scikits.appspot.com/scikits) out there. This scikit contains modules specifically for machine learning and data mining, which explains the second component of the library name. :)\n",
    "\n",
    "To load in the data, you import the module `datasets` from `sklearn`. Then, you can use the `load_digits()` method from `datasets` to load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `datasets` from `sklearn`\n",
    "\n",
    "\n",
    "# Load in the `digits` data using load_digits() as `digits`\n",
    "\n",
    "\n",
    "# Print the `digits` data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit background of the DIGITS dataset: Did you notice that the dataset has `64` features? That is because the dataset contains processed images of hand-written digits. These images are `8x8` in dimensions. By processing, the value of an instance under each feature is the gray-scale of that cell in the original image. Do not worry we will restore some of these images later on.\n",
    "\n",
    "Fun fact: the MNIST dataset used with deep learning contains hand-written digits in 28x28 dimensions (784 features). \n",
    "\n",
    "The class/target of the dataset contains `[0, 9]` a total of 10 numbers. The idea is that any image is corresponding to a number in `[0, 9]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "When first starting out with a data set, it’s always a good idea to go through the data description and see what you can already learn. When it comes to `scikit-learn`, you don’t immediately have this information readily available, but in the case where you import data from another source, there's usually a data description present, which will already be a sufficient amount of information to gather some insights into your data.\n",
    "\n",
    "However, these insights are not merely deep enough for the analysis that you are going to perform. You really need to have a good working knowledge about the data set.\n",
    "\n",
    "Performing an exploratory data analysis (EDA) on a data set like the one that this tutorial now has might seem difficult.\n",
    "\n",
    "Where do you start exploring these handwritten digits?\n",
    "\n",
    "### Gathering Basic Information on Your Data\n",
    "Let’s say that you haven’t checked any data description folder (or maybe you want to double-check the information that has been given to you).\n",
    "\n",
    "Then you should start with gathering the basic information.\n",
    "\n",
    "When you printed out the `digits` data after having loaded it with the help of the `scikit-learn` `datasets` module, you will have noticed that there is already a lot of information available. You already have knowledge of things such as the target values and the description of your data. You can access the `digits` data through the attribute `data`. Similarly, you can also access the target values or labels through the target attribute and the description through the `DESCR` attribute.\n",
    "\n",
    "To see which keys you have available to already get to know your data, you can just run `digits.keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the keys of the `digits` data\n",
    "\n",
    "\n",
    "# Print out the data\n",
    "\n",
    "\n",
    "# Print out the target values\n",
    "\n",
    "\n",
    "# Print out the description `DESCR` of the `digits` data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the images, say first three, using `digits.images[:3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(digits.images[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait... We were expecting images, right? What are these numbers and matrices again? Remember these are processed images, you will need tools to restore them back to their original form.\n",
    "\n",
    "The next thing that you can (double)check is the type of your data.\n",
    "\n",
    "If you used `read_csv()` to import the data, you would have had a data frame that contains just the data. There wouldn’t be any description component, but you would be able to resort to, for example, `head()` or `tail()` to inspect your data. In these cases, it’s always wise to read up on the data description folder!\n",
    "\n",
    "However, this tutorial assumes that you make use of the library's data and the type of the digits variable is not that straightforward if you’re not familiar with the library. Look at the print out in the first code chunk. You’ll see that digits actually contains `numpy arrays`!\n",
    "\n",
    "This is already quite some important information. But how do you access these arays?\n",
    "\n",
    "It’s very easy, actually: you use attributes to access the relevant arrays.\n",
    "\n",
    "Remember that you have already seen which attributes are available when you printed `digits.keys()`. For instance, you have the `data` attribute to isolate the data, `target` to see the target values and the `DESCR` for the description, …\n",
    "\n",
    "But what then?\n",
    "\n",
    "The first thing that you should know of an array is its shape. That is, the number of dimensions and items that is contained within an array. The array’s shape is a tuple of integers that specify the sizes of each dimension. In other words, if you have a 3d array like this y = np.zeros((2, 3, 4)), the shape of your array will be (2,3,4).\n",
    "\n",
    "Now let’s try to see what the shape is of these three arrays that you have distinguished (the `data`, `target` and `DESCR` arrays).\n",
    "\n",
    "Use first the `data` attribute to isolate the numpy array from the digits data and then use the `shape` attribute to find out more. You can do the same for the `target` and `DESCR`. Again, we will deal with the `images` attribute later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import `numpy` as `np`\n",
    "import numpy as np\n",
    "\n",
    "# Isolate the `digits` data\n",
    "digits_data = digits.data\n",
    "\n",
    "# Inspect the shape\n",
    "print('data dimensions: ', digits_data.shape)\n",
    "\n",
    "# Isolate the target values with `target`\n",
    "digits_target = digits.target\n",
    "\n",
    "# Inspect the shape\n",
    "print('target dimensions: ', digits_target.shape)\n",
    "\n",
    "# Print the number of unique labels\n",
    "number_digits = len(np.unique(digits.target))\n",
    "\n",
    "# Isolate the `images`\n",
    "digits_images = digits.images\n",
    "\n",
    "# Inspect the shape\n",
    "print('image dimensions: ', digits_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the question: \n",
    "\n",
    "Use your `numpy` knowledege, and other knowledge you gained in this class, to interpret what above dimensions mean. For instance, we already talked about the `64` in the `data` dimensions means each instance in our dataset has `64` features.\n",
    "\n",
    "Provide your interpretations in the block below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Double click and type your answers here__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you may notice is taht the `target` attribute only have `1` dimension value, what does that mean?\n",
    "\n",
    "But all those target values contain `10` unique values, namely, from 0 to 9. In other words, all `1797` target values are made up of numbers that lie between 0 and 9. This means that the digits that your model will need to recognize are numbers from 0 to 9.\n",
    "\n",
    "For the `images` attributs, you can visually check that the images and the data are related by reshaping the images array to two dimensions: `digits.images.reshape((1797, 64))`.\n",
    "\n",
    "But if you want to be completely sure, better to check with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.all(digits.images.reshape((1797,64)) == digits.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `numpy` method `all()`, you test whether all array elements along a given axis evaluate to `True`. In this case, you evaluate if it’s true that the reshaped `images` array equals `digits.data`. You’ll see that the result will be `True` in this case.\n",
    "\n",
    "### Visualize Your Data Images With matplotlib\n",
    "Then, you can take your exploration up a notch by visualizing the images that you’ll be working with. You can use one of Python’s data visualization libraries, such as `matplotlib`, for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Figure size (width, height) in inches\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Adjust the subplots \n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# For each of the 64 images\n",
    "for i in range(64):\n",
    "    # Initialize the subplots: add a subplot in the grid of 8 by 8, at the i+1-th position\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    # Display an image at the i-th position\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code chunk seems quite lengthy at first sight and this might be overwhelming. But, what happens in the code chunk above is actually pretty simple once you break it down into parts:\n",
    "\n",
    "- You import `matplotlib.pyplot`.\n",
    "- Next, you set up a figure `fig` with a figure size of `6` inches wide and `6` inches long. This is your blank canvas where all the subplots with the images will appear.\n",
    "- Then you go to the level of the subplots to adjust some parameters: you set the left side of the suplots of the figure to `0`, the right side of the suplots of the figure to `1`, the bottom to `0` and the top to `1`. The height of the blank space between the suplots is set at `0.005` and the width is set at `0.05`. These are merely layout adjustments.\n",
    "- After that, you start filling up the figure that you have made with the help of a `for` loop.\n",
    "- You initialize the suplots one by one, adding one at each position in the grid that is 8 by 8 images big.\n",
    "- You display each time one of the images at each position in the grid. As a color map, you take binary colors, which in this case will result in gray-scale (black, gray values and white colors, __higher value means darker color__). The interpolation method that you use is 'nearest', which means that your data is interpolated in such a way that it isn’t smooth. You can see the effect of the different interpolation methods [here](http://matplotlib.org/examples/images_contours_and_fields/interpolation_methods.html).\n",
    "- The cherry on the pie is the addition of text to your subplots. The target labels are printed at coordinates (0,7) of each subplot, which in practice means that they will appear in the bottom-left of each of the subplots.\n",
    "- Don’t forget to show the plot with `plt.show()`!\n",
    "\n",
    "On a more simple note, you can also visualize the target labels with an image, just like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join the images and target labels in a list\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "\n",
    "# for every element in the list\n",
    "for index, (image, label) in enumerate(images_and_labels[:8]):\n",
    "    # initialize a subplot of 2X4 at the i+1-th position\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    # Don't plot any axes\n",
    "    plt.axis('off')\n",
    "    # Display images in all subplots \n",
    "    plt.imshow(image, cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "    # Add a title to each subplot\n",
    "    plt.title('Training: ' + str(label))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, you zip the two `numpy` arrays (`images` and `target`, aka. `labels`) together and save it into a variable called `images_and_labels`. You’ll see now that this list contains suples of each time an instance of `digits.images` and a corresponding `digits.target` value.\n",
    "\n",
    "Then, you say that for the first eight elements of `images_and_labels` -note that the index starts at 0!-, you initialize subplots in a grid of `2` by `4` at each position. You turn of the plotting of the axes and you display images in all the subplots with a color map `plt.cm.gray_r` (which returns all grey colors) and the interpolation method used is `nearest`. You give a title to each subplot, and you show it.\n",
    "\n",
    "Not too hard, huh?\n",
    "\n",
    "And now you know a very good idea of the data that you’ll be working with!\n",
    "\n",
    "### Visualizing Your Data: Principal Component Analysis (PCA)\n",
    "\n",
    "But is there no other way to visualize the data?\n",
    "\n",
    "As the digits data set contains 64 features, this might prove to be a challenging task. You can imagine that it’s very hard to understand the structure and keep the overview of the digits data. In such cases, it is said that you’re working with a __high dimensional__ data set.\n",
    "\n",
    "High dimensionality of data is a direct result of trying to describe the objects via a collection of features. Other examples of high dimensional data are, for example, financial data, climate data, neuroimaging, survey data …\n",
    "\n",
    "But, as you might have gathered already, this is not always __easy__. In some cases, high dimensionality can be problematic, as your algorithms will need to take into account too many features. In such cases, you speak of the __curse of dimensionality__. Because having a lot of dimensions can also mean that your data points are __far away__ from virtually every other point, which makes the distances between the data points uninformative. In other words, you will need __much more__ data points to find reasonable patterns from your data.\n",
    "\n",
    "But don’t worry, though, because the __curse of dimensionality__ is not simply a matter of counting the number of features. There are also cases in which the effective dimensionality might be much smaller than the number of the features, such as in data sets where some features are irrelevant. In other words, it is __safe__ to filter out some of the features that are not as useful.\n",
    "\n",
    "In addition, you can also understand that data with only two or three dimensions is easier to grasp and can also be visualized easily.\n",
    "\n",
    "That all explains why you’re going to visualize the data with the help of one of the __Dimensionality Reduction__ techniques, namely __Principal Component Analysis (PCA)__. The idea in PCA is to find a *linear combination* of the  variables that contains most of the information. This new variable or “principal component” can replace the  original variables.\n",
    "\n",
    "In short, it’s a linear transformation method that yields the directions (principal components) that maximize the __variance__ of the data. Remember that the variance indicates how far a set of data points lie apart. If you want to know more, go to [this page](http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca).\n",
    "\n",
    "You can easily apply PCA do your data with the help of `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import  `PCA` from `sklearn.decomposition`\n",
    "\n",
    "\n",
    "# Create a Randomized PCA model that takes two components\n",
    "# name the model as `randomized_pca`, using the `PCA` function \n",
    "# with the parameter of `n_components` set to 2 and `svd_solver` set to `randomized`\n",
    "\n",
    "\n",
    "# Fit and transform the data to the model\n",
    "# using the `fit_transform` function and name the result as `reduced_data_rpca`\n",
    "\n",
    "\n",
    "# Create a regular PCA model \n",
    "# name the model as `pca`, using the `PCA` function with the parameter of `n_components` set to 2\n",
    "\n",
    "\n",
    "# Fit and transform the data to the model\n",
    "# using the `fit_transform` function and name the result as `reduced_data_pca`\n",
    "\n",
    "\n",
    "# Inspect the shape of `reduced_data_pca`\n",
    "\n",
    "\n",
    "# Print out both reduced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we build a randomized PCA model is that it typically perform better than the regular PCA model. Try replacing the randomized PCA model or estimator object with a regular PCA model and see what the difference is.\n",
    "\n",
    "Note how you explicitly tell the model to only keep `2` components. This is to make sure that you have two-dimensional data to plot. Also, note that you don’t pass the target class with the labels to the PCA transformation because you want to investigate if the PCA reveals the distribution of the different labels and if you can clearly separate the instances from each other.\n",
    "\n",
    "You can now build a scatterplot to visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the colors for different classes/labels [0, 9]\n",
    "colors = ['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', 'orange', 'gray']\n",
    "\n",
    "# We are using the two components in the `randomized_rpca` model as x-axis and y-axis in the plot\n",
    "# Note that we are plotting the data points, not the original figures\n",
    "# The distance in the plot actually means the distance between the data points\n",
    "# i.e. two data points of class (0) should be close to each other\n",
    "for i in range(len(colors)):\n",
    "    # x-axis is the first component in `reduced_data_rpca` model\n",
    "    # and the color is corresponding to class, e.g., 0 is `black`\n",
    "    x = reduced_data_rpca[:, 0][digits.target == i]\n",
    "    # x-axis is the second component in `reduced_data_rpca` model\n",
    "    y = reduced_data_rpca[:, 1][digits.target == i]\n",
    "    # plot it out\n",
    "    plt.scatter(x, y, c=colors[i])\n",
    "# add legend as class names\n",
    "plt.legend(digits.target_names, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# add x-axis name as 'First Principal Component'\n",
    "plt.xlabel('First Principal Component')\n",
    "# add y-axis name as 'Second Principal Component'\n",
    "plt.ylabel('Second Principal Component')\n",
    "# add title of plot as \"random PCA Scatter Plot\"\n",
    "plt.title(\"random PCA Scatter Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again you use `matplotlib` to visualize the data. It’s good for a quick visualization of what you’re working with, but you might have to consider something a little bit more fancy if you’re working on making this part of your data science portfolio.\n",
    "\n",
    "Also note that the last call to show the plot (`plt.show()`) is not necessary if you’re working in Jupyter Notebook, as you’ll want to put the images inline (using magic command `%matplotlib inline`). When in doubt, you can always check out our [Definitive Guide to Jupyter Notebook](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook/).\n",
    "\n",
    "What happens in the code chunk above is the following:\n",
    "\n",
    "1. You put your colors together in a list. Note that you list ten colors, which is equal to the number of labels that you have. This way, you make sure that your data points can be colored in according to the labels. Then, you set up a range that goes from 0 to 10. Mind you that this range is not inclusive! Remember that this is the same for indices of a list, for example.\n",
    "2. You set up your `x` and `y` coordinates. You take the first or the second column of `reduced_data_rpca`, and you select only those data points for which the label equals the index that you’re considering. That means that in the first run, you’ll consider the data points with label `0`, then label `1`, … and so on.\n",
    "3. You construct the scatter plot. Fill in the `x` and `y` coordinates and assign a color to the batch that you’re processing. The first run, you’ll give the color black to all data points, the next run blue, … and so on.\n",
    "4. You add a legend to your scatter plot. Use the `target_names` key to get the right labels for your data points.\n",
    "5. Add labels to your `x` and `y` axes that are meaningful.\n",
    "5. Reveal the resulting plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO IT YOURSELF\n",
    "As you remember, we also build a regular PCA model `pca` using the digits data - which results in as `reduced_data_pca`. Use this to replace `reduced_data_rpca` in above code block and visualize the results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complete the code below\n",
    "# you will use the same `colors` list so you do not have to define it again\n",
    "\n",
    "\n",
    "# We are using the two components in the `randomized_pca` model as x-axis and y-axis in the plot\n",
    "# Note that we are plotting the data points, not the original figures\n",
    "# The distance in the plot actually means the distance between the data points\n",
    "# i.e. two data points of class (0) should be close to each other\n",
    "\n",
    "    # x-axis is the first component in `reduced_data_pca` model\n",
    "    # and the color is corresponding to class, e.g., 0 is `black`\n",
    "    \n",
    "    # x-axis is the second component in `reduced_data_pca` model\n",
    "    \n",
    "    # plot it out\n",
    "    \n",
    "# add legend as class names\n",
    "\n",
    "# add x-axis name as 'First Principal Component'\n",
    "\n",
    "# add y-axis name as 'Second Principal Component'\n",
    "\n",
    "# add title of plot as \"PCA Scatter Plot\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the question\n",
    "\n",
    "Can you observe the difference between using the `PCA` and `random PCA` models? Use the next code block to provide your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Double click and type answer here__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save both `reduced_data_pca` and `reduced_data_rpca` models to disk so that we can re-use them in Part 2 of this mini project. Code below does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../data/reduced_data_pca.npy', reduced_data_pca)\n",
    "np.save('../data/reduced_data_rpca.npy', reduced_data_rpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for part 1. Please make sure your sync the complete notebook to your github repo for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
